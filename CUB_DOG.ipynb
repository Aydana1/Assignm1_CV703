{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09b89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "sz = 448\n",
    "test_transform=transforms.Compose([\n",
    "                    transforms.Resize((sz, sz)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "\n",
    "from new_loader import CUBDataset1, DOGDataset1\n",
    "\n",
    "\n",
    "def cub_and_dogs(cub_root = \"./CUB_200_2011\",\n",
    "                 dog_root = \"./dog/dog\",\n",
    "                data_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                                                    ]),\n",
    "                 bs=128\n",
    "                 ):\n",
    "\n",
    "\n",
    "    train_dataset_dog = DOGDataset1(image_root_path=f\"{dog_root}\", transform=data_transform, split=\"train\")\n",
    "    lengths = [int(len(train_dataset_dog) * 0.9), int(len(train_dataset_dog) * 0.1)]\n",
    "    torch.manual_seed(0)\n",
    "    train_set_dog, val_set_dog = random_split(train_dataset_dog, lengths)\n",
    "    test_dataset_dog = DOGDataset1(image_root_path=f\"{dog_root}\", transform=test_transform, split=\"test\")\n",
    "\n",
    "    train_dataset_cub = CUBDataset1(image_root_path=f\"{cub_root}\", transform=data_transform, split=\"train\", concat=True)\n",
    "    lengths = [int(len(train_dataset_cub) * 0.9), int(len(train_dataset_cub) * 0.1) + 1]\n",
    "    torch.manual_seed(0)\n",
    "    train_set_cub, val_set_cub = random_split(train_dataset_cub, lengths)\n",
    "    test_dataset_cub = CUBDataset1(image_root_path=f\"{cub_root}\", transform=test_transform, split=\"test\", concat=True)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "                 torch.utils.data.ConcatDataset([train_set_dog, train_set_cub]),\n",
    "                 batch_size=bs, shuffle=True,\n",
    "                 num_workers=1, pin_memory=True)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.ConcatDataset([val_set_dog, val_set_cub ]),\n",
    "        batch_size=bs, shuffle=True,\n",
    "        num_workers=1, pin_memory=True)\n",
    "\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "                 torch.utils.data.ConcatDataset([test_dataset_dog, test_dataset_cub]),\n",
    "                 batch_size=bs, shuffle=False,\n",
    "                 num_workers=1, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f4189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = cub_and_dogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6986e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 15, 113)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader),  len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215abbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetV2(\n",
       "  (stem): Sequential(\n",
       "    (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (pad): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 2048, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 2048, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): GroupNormAct(\n",
       "    32, 2048, eps=1e-05, affine=True\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "    (fc): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm \n",
    "\n",
    "model = timm.create_model('resnetv2_50x1_bitm', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529ce873",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1)),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74324d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetV2(\n",
       "  (stem): Sequential(\n",
       "    (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (pad): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 64, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 128, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 256, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ResNetStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): PreActBottleneck(\n",
       "          (downsample): DownsampleConv(\n",
       "            (conv): StdConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 1024, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 2048, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): PreActBottleneck(\n",
       "          (norm1): GroupNormAct(\n",
       "            32, 2048, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv1): StdConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm3): GroupNormAct(\n",
       "            32, 512, eps=1e-05, affine=True\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): GroupNormAct(\n",
       "    32, 2048, eps=1e-05, affine=True\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (1): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "364e4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25a66ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "        ct = 0\n",
    "        for child in model.children():\n",
    "            ct += 1\n",
    "            if ct < 4:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "set_parameter_requires_grad(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a22152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 655,680 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706bbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab7103d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/127] Train Loss: 5.5031\n",
      "Epoch [1/50], Step [20/127] Train Loss: 4.9878\n",
      "Epoch [1/50], Step [30/127] Train Loss: 4.7071\n",
      "Epoch [1/50], Step [40/127] Train Loss: 4.2701\n",
      "Epoch [1/50], Step [50/127] Train Loss: 4.0466\n",
      "Epoch [1/50], Step [60/127] Train Loss: 3.5301\n",
      "Epoch [1/50], Step [70/127] Train Loss: 3.2696\n",
      "Epoch [1/50], Step [80/127] Train Loss: 2.8513\n",
      "Epoch [1/50], Step [90/127] Train Loss: 2.8932\n",
      "Epoch [1/50], Step [100/127] Train Loss: 2.6899\n",
      "Epoch [1/50], Step [110/127] Train Loss: 2.5654\n",
      "Epoch [1/50], Step [120/127] Train Loss: 2.4931\n",
      "Train loss at epoch 1 is 3.7097\n",
      "Train acc at epoch 1 is 24.9599\n",
      "Val loss at epoch 1 is 0.1654\n",
      "Val acc at epoch 1 is 27.5314\n",
      "Epoch [2/50], Step [10/127] Train Loss: 1.9847\n",
      "Epoch [2/50], Step [20/127] Train Loss: 2.0108\n",
      "Epoch [2/50], Step [30/127] Train Loss: 2.0796\n",
      "Epoch [2/50], Step [40/127] Train Loss: 1.8342\n",
      "Epoch [2/50], Step [50/127] Train Loss: 1.7639\n",
      "Epoch [2/50], Step [60/127] Train Loss: 1.6339\n",
      "Epoch [2/50], Step [70/127] Train Loss: 1.4474\n",
      "Epoch [2/50], Step [80/127] Train Loss: 1.5097\n",
      "Epoch [2/50], Step [90/127] Train Loss: 1.4908\n",
      "Epoch [2/50], Step [100/127] Train Loss: 1.4748\n",
      "Epoch [2/50], Step [110/127] Train Loss: 1.4158\n",
      "Epoch [2/50], Step [120/127] Train Loss: 1.4167\n",
      "Train loss at epoch 2 is 1.6798\n",
      "Train acc at epoch 2 is 61.9489\n",
      "Val loss at epoch 2 is 0.1432\n",
      "Val acc at epoch 2 is 62.2319\n",
      "Epoch [3/50], Step [10/127] Train Loss: 1.1864\n",
      "Epoch [3/50], Step [20/127] Train Loss: 1.0975\n",
      "Epoch [3/50], Step [30/127] Train Loss: 1.0880\n",
      "Epoch [3/50], Step [40/127] Train Loss: 1.2322\n",
      "Epoch [3/50], Step [50/127] Train Loss: 1.1033\n",
      "Epoch [3/50], Step [60/127] Train Loss: 1.0515\n",
      "Epoch [3/50], Step [70/127] Train Loss: 1.0819\n",
      "Epoch [3/50], Step [80/127] Train Loss: 1.1006\n",
      "Epoch [3/50], Step [90/127] Train Loss: 1.0994\n",
      "Epoch [3/50], Step [100/127] Train Loss: 1.0665\n",
      "Epoch [3/50], Step [110/127] Train Loss: 1.0383\n",
      "Epoch [3/50], Step [120/127] Train Loss: 1.0040\n",
      "Train loss at epoch 3 is 1.1402\n",
      "Train acc at epoch 3 is 73.2000\n",
      "Val loss at epoch 3 is 0.0318\n",
      "Val acc at epoch 3 is 72.9966\n",
      "Epoch [4/50], Step [10/127] Train Loss: 0.9304\n",
      "Epoch [4/50], Step [20/127] Train Loss: 0.9108\n",
      "Epoch [4/50], Step [30/127] Train Loss: 0.8610\n",
      "Epoch [4/50], Step [40/127] Train Loss: 0.9548\n",
      "Epoch [4/50], Step [50/127] Train Loss: 0.8663\n",
      "Epoch [4/50], Step [60/127] Train Loss: 0.8541\n",
      "Epoch [4/50], Step [70/127] Train Loss: 0.8703\n",
      "Epoch [4/50], Step [80/127] Train Loss: 0.8335\n",
      "Epoch [4/50], Step [90/127] Train Loss: 0.7687\n",
      "Epoch [4/50], Step [100/127] Train Loss: 0.9457\n",
      "Epoch [4/50], Step [110/127] Train Loss: 0.8246\n",
      "Epoch [4/50], Step [120/127] Train Loss: 0.9079\n",
      "Train loss at epoch 4 is 0.8967\n",
      "Train acc at epoch 4 is 78.6217\n",
      "Val loss at epoch 4 is 0.1087\n",
      "Val acc at epoch 4 is 78.0983\n",
      "Epoch [5/50], Step [10/127] Train Loss: 0.6450\n",
      "Epoch [5/50], Step [20/127] Train Loss: 0.7133\n",
      "Epoch [5/50], Step [30/127] Train Loss: 0.8176\n",
      "Epoch [5/50], Step [40/127] Train Loss: 0.6776\n",
      "Epoch [5/50], Step [50/127] Train Loss: 0.8075\n",
      "Epoch [5/50], Step [60/127] Train Loss: 0.6398\n",
      "Epoch [5/50], Step [70/127] Train Loss: 0.6683\n",
      "Epoch [5/50], Step [80/127] Train Loss: 0.6691\n",
      "Epoch [5/50], Step [90/127] Train Loss: 0.7678\n",
      "Epoch [5/50], Step [100/127] Train Loss: 0.6381\n",
      "Epoch [5/50], Step [110/127] Train Loss: 0.7020\n",
      "Epoch [5/50], Step [120/127] Train Loss: 0.6274\n",
      "Train loss at epoch 5 is 0.7530\n",
      "Train acc at epoch 5 is 81.7216\n",
      "Val loss at epoch 5 is 0.0691\n",
      "Val acc at epoch 5 is 80.9992\n",
      "Epoch [6/50], Step [10/127] Train Loss: 0.6975\n",
      "Epoch [6/50], Step [20/127] Train Loss: 0.6456\n",
      "Epoch [6/50], Step [30/127] Train Loss: 0.7276\n",
      "Epoch [6/50], Step [40/127] Train Loss: 0.6804\n",
      "Epoch [6/50], Step [50/127] Train Loss: 0.6622\n",
      "Epoch [6/50], Step [60/127] Train Loss: 0.7153\n",
      "Epoch [6/50], Step [70/127] Train Loss: 0.6112\n",
      "Epoch [6/50], Step [80/127] Train Loss: 0.6792\n",
      "Epoch [6/50], Step [90/127] Train Loss: 0.7318\n",
      "Epoch [6/50], Step [100/127] Train Loss: 0.6806\n",
      "Epoch [6/50], Step [110/127] Train Loss: 0.5593\n",
      "Epoch [6/50], Step [120/127] Train Loss: 0.7179\n",
      "Train loss at epoch 6 is 0.6525\n",
      "Train acc at epoch 6 is 84.2349\n",
      "Val loss at epoch 6 is 0.0561\n",
      "Val acc at epoch 6 is 83.3055\n",
      "Epoch [7/50], Step [10/127] Train Loss: 0.5405\n",
      "Epoch [7/50], Step [20/127] Train Loss: 0.5661\n",
      "Epoch [7/50], Step [30/127] Train Loss: 0.5117\n",
      "Epoch [7/50], Step [40/127] Train Loss: 0.4862\n",
      "Epoch [7/50], Step [50/127] Train Loss: 0.4597\n",
      "Epoch [7/50], Step [60/127] Train Loss: 0.5701\n",
      "Epoch [7/50], Step [70/127] Train Loss: 0.6043\n",
      "Epoch [7/50], Step [80/127] Train Loss: 0.7002\n",
      "Epoch [7/50], Step [90/127] Train Loss: 0.6488\n",
      "Epoch [7/50], Step [100/127] Train Loss: 0.6262\n",
      "Epoch [7/50], Step [110/127] Train Loss: 0.5062\n",
      "Epoch [7/50], Step [120/127] Train Loss: 0.5517\n",
      "Train loss at epoch 7 is 0.5803\n",
      "Train acc at epoch 7 is 85.7910\n",
      "Val loss at epoch 7 is 0.0608\n",
      "Val acc at epoch 7 is 84.8227\n",
      "Epoch [8/50], Step [10/127] Train Loss: 0.5769\n",
      "Epoch [8/50], Step [20/127] Train Loss: 0.5043\n",
      "Epoch [8/50], Step [30/127] Train Loss: 0.6123\n",
      "Epoch [8/50], Step [40/127] Train Loss: 0.5568\n",
      "Epoch [8/50], Step [50/127] Train Loss: 0.6107\n",
      "Epoch [8/50], Step [60/127] Train Loss: 0.5973\n",
      "Epoch [8/50], Step [70/127] Train Loss: 0.6971\n",
      "Epoch [8/50], Step [80/127] Train Loss: 0.4926\n",
      "Epoch [8/50], Step [90/127] Train Loss: 0.5115\n",
      "Epoch [8/50], Step [100/127] Train Loss: 0.5893\n",
      "Epoch [8/50], Step [110/127] Train Loss: 0.5962\n",
      "Epoch [8/50], Step [120/127] Train Loss: 0.4393\n",
      "Train loss at epoch 8 is 0.5227\n",
      "Train acc at epoch 8 is 87.2113\n",
      "Val loss at epoch 8 is 0.0337\n",
      "Val acc at epoch 8 is 86.1621\n",
      "Epoch [9/50], Step [10/127] Train Loss: 0.4677\n",
      "Epoch [9/50], Step [20/127] Train Loss: 0.5512\n",
      "Epoch [9/50], Step [30/127] Train Loss: 0.4679\n",
      "Epoch [9/50], Step [40/127] Train Loss: 0.5096\n",
      "Epoch [9/50], Step [50/127] Train Loss: 0.4759\n",
      "Epoch [9/50], Step [60/127] Train Loss: 0.4535\n",
      "Epoch [9/50], Step [70/127] Train Loss: 0.4915\n",
      "Epoch [9/50], Step [80/127] Train Loss: 0.4060\n",
      "Epoch [9/50], Step [90/127] Train Loss: 0.4480\n",
      "Epoch [9/50], Step [100/127] Train Loss: 0.4630\n",
      "Epoch [9/50], Step [110/127] Train Loss: 0.5183\n",
      "Epoch [9/50], Step [120/127] Train Loss: 0.4866\n",
      "Train loss at epoch 9 is 0.4741\n",
      "Train acc at epoch 9 is 88.5081\n",
      "Val loss at epoch 9 is 0.0480\n",
      "Val acc at epoch 9 is 87.3958\n",
      "Epoch [10/50], Step [10/127] Train Loss: 0.4195\n",
      "Epoch [10/50], Step [20/127] Train Loss: 0.4911\n",
      "Epoch [10/50], Step [30/127] Train Loss: 0.4869\n",
      "Epoch [10/50], Step [40/127] Train Loss: 0.5050\n",
      "Epoch [10/50], Step [50/127] Train Loss: 0.3669\n",
      "Epoch [10/50], Step [60/127] Train Loss: 0.3691\n",
      "Epoch [10/50], Step [70/127] Train Loss: 0.4016\n",
      "Epoch [10/50], Step [80/127] Train Loss: 0.5608\n",
      "Epoch [10/50], Step [90/127] Train Loss: 0.4605\n",
      "Epoch [10/50], Step [100/127] Train Loss: 0.4526\n",
      "Epoch [10/50], Step [110/127] Train Loss: 0.3722\n",
      "Epoch [10/50], Step [120/127] Train Loss: 0.4213\n",
      "Train loss at epoch 10 is 0.4344\n",
      "Train acc at epoch 10 is 89.7246\n",
      "Val loss at epoch 10 is 0.0596\n",
      "Val acc at epoch 10 is 88.4072\n",
      "Epoch [11/50], Step [10/127] Train Loss: 0.4837\n",
      "Epoch [11/50], Step [20/127] Train Loss: 0.5144\n",
      "Epoch [11/50], Step [30/127] Train Loss: 0.5487\n",
      "Epoch [11/50], Step [40/127] Train Loss: 0.3499\n",
      "Epoch [11/50], Step [50/127] Train Loss: 0.3667\n",
      "Epoch [11/50], Step [60/127] Train Loss: 0.4115\n",
      "Epoch [11/50], Step [70/127] Train Loss: 0.4301\n",
      "Epoch [11/50], Step [80/127] Train Loss: 0.3835\n",
      "Epoch [11/50], Step [90/127] Train Loss: 0.3363\n",
      "Epoch [11/50], Step [100/127] Train Loss: 0.3711\n",
      "Epoch [11/50], Step [110/127] Train Loss: 0.3858\n",
      "Epoch [11/50], Step [120/127] Train Loss: 0.4077\n",
      "Train loss at epoch 11 is 0.3864\n",
      "Train acc at epoch 11 is 91.4969\n",
      "Val loss at epoch 11 is 0.0272\n",
      "Val acc at epoch 11 is 90.1189\n",
      "Epoch [12/50], Step [10/127] Train Loss: 0.4799\n",
      "Epoch [12/50], Step [20/127] Train Loss: 0.3139\n",
      "Epoch [12/50], Step [30/127] Train Loss: 0.4068\n",
      "Epoch [12/50], Step [40/127] Train Loss: 0.4989\n",
      "Epoch [12/50], Step [50/127] Train Loss: 0.3052\n",
      "Epoch [12/50], Step [60/127] Train Loss: 0.3399\n",
      "Epoch [12/50], Step [70/127] Train Loss: 0.3880\n",
      "Epoch [12/50], Step [80/127] Train Loss: 0.3757\n",
      "Epoch [12/50], Step [90/127] Train Loss: 0.4211\n",
      "Epoch [12/50], Step [100/127] Train Loss: 0.3692\n",
      "Epoch [12/50], Step [110/127] Train Loss: 0.3868\n",
      "Epoch [12/50], Step [120/127] Train Loss: 0.4866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch 12 is 0.3731\n",
      "Train acc at epoch 12 is 92.0403\n",
      "Val loss at epoch 12 is 0.0242\n",
      "Val acc at epoch 12 is 90.6024\n",
      "Epoch [13/50], Step [10/127] Train Loss: 0.3995\n",
      "Epoch [13/50], Step [20/127] Train Loss: 0.3040\n",
      "Epoch [13/50], Step [30/127] Train Loss: 0.4181\n",
      "Epoch [13/50], Step [40/127] Train Loss: 0.3186\n",
      "Epoch [13/50], Step [50/127] Train Loss: 0.4151\n",
      "Epoch [13/50], Step [60/127] Train Loss: 0.3588\n",
      "Epoch [13/50], Step [70/127] Train Loss: 0.3821\n",
      "Epoch [13/50], Step [80/127] Train Loss: 0.3480\n",
      "Epoch [13/50], Step [90/127] Train Loss: 0.2903\n",
      "Epoch [13/50], Step [100/127] Train Loss: 0.3824\n",
      "Epoch [13/50], Step [110/127] Train Loss: 0.4116\n",
      "Epoch [13/50], Step [120/127] Train Loss: 0.4365\n",
      "Train loss at epoch 13 is 0.3639\n",
      "Train acc at epoch 13 is 92.1205\n",
      "Val loss at epoch 13 is 0.0430\n",
      "Val acc at epoch 13 is 90.7025\n",
      "Epoch [14/50], Step [10/127] Train Loss: 0.4356\n",
      "Epoch [14/50], Step [20/127] Train Loss: 0.2728\n",
      "Epoch [14/50], Step [30/127] Train Loss: 0.2515\n",
      "Epoch [14/50], Step [40/127] Train Loss: 0.4143\n",
      "Epoch [14/50], Step [50/127] Train Loss: 0.3651\n",
      "Epoch [14/50], Step [60/127] Train Loss: 0.4057\n",
      "Epoch [14/50], Step [70/127] Train Loss: 0.2672\n",
      "Epoch [14/50], Step [80/127] Train Loss: 0.2959\n",
      "Epoch [14/50], Step [90/127] Train Loss: 0.3512\n",
      "Epoch [14/50], Step [100/127] Train Loss: 0.4527\n",
      "Epoch [14/50], Step [110/127] Train Loss: 0.3508\n",
      "Epoch [14/50], Step [120/127] Train Loss: 0.3312\n",
      "Train loss at epoch 14 is 0.3543\n",
      "Train acc at epoch 14 is 92.5157\n",
      "Val loss at epoch 14 is 0.1153\n",
      "Val acc at epoch 14 is 91.0526\n",
      "Epoch [15/50], Step [10/127] Train Loss: 0.2950\n",
      "Epoch [15/50], Step [20/127] Train Loss: 0.4338\n",
      "Epoch [15/50], Step [30/127] Train Loss: 0.3478\n",
      "Epoch [15/50], Step [40/127] Train Loss: 0.2865\n",
      "Epoch [15/50], Step [50/127] Train Loss: 0.3202\n",
      "Epoch [15/50], Step [60/127] Train Loss: 0.3389\n",
      "Epoch [15/50], Step [70/127] Train Loss: 0.3145\n",
      "Epoch [15/50], Step [80/127] Train Loss: 0.2915\n",
      "Epoch [15/50], Step [90/127] Train Loss: 0.3715\n",
      "Epoch [15/50], Step [100/127] Train Loss: 0.3432\n",
      "Epoch [15/50], Step [110/127] Train Loss: 0.3187\n",
      "Epoch [15/50], Step [120/127] Train Loss: 0.2642\n",
      "Train loss at epoch 15 is 0.3443\n",
      "Train acc at epoch 15 is 92.5466\n",
      "Val loss at epoch 15 is 0.0652\n",
      "Val acc at epoch 15 is 91.1137\n",
      "Epoch [16/50], Step [10/127] Train Loss: 0.2918\n",
      "Epoch [16/50], Step [20/127] Train Loss: 0.3062\n",
      "Epoch [16/50], Step [30/127] Train Loss: 0.3862\n",
      "Epoch [16/50], Step [40/127] Train Loss: 0.4048\n",
      "Epoch [16/50], Step [50/127] Train Loss: 0.3337\n",
      "Epoch [16/50], Step [60/127] Train Loss: 0.4259\n",
      "Epoch [16/50], Step [70/127] Train Loss: 0.4312\n",
      "Epoch [16/50], Step [80/127] Train Loss: 0.3274\n",
      "Epoch [16/50], Step [90/127] Train Loss: 0.4813\n",
      "Epoch [16/50], Step [100/127] Train Loss: 0.3381\n",
      "Epoch [16/50], Step [110/127] Train Loss: 0.3524\n",
      "Epoch [16/50], Step [120/127] Train Loss: 0.3366\n",
      "Train loss at epoch 16 is 0.3343\n",
      "Train acc at epoch 16 is 93.0036\n",
      "Val loss at epoch 16 is 0.1148\n",
      "Val acc at epoch 16 is 91.4972\n",
      "Epoch [17/50], Step [10/127] Train Loss: 0.3575\n",
      "Epoch [17/50], Step [20/127] Train Loss: 0.2574\n",
      "Epoch [17/50], Step [30/127] Train Loss: 0.3196\n",
      "Epoch [17/50], Step [40/127] Train Loss: 0.2584\n",
      "Epoch [17/50], Step [50/127] Train Loss: 0.3798\n",
      "Epoch [17/50], Step [60/127] Train Loss: 0.2522\n",
      "Epoch [17/50], Step [70/127] Train Loss: 0.3932\n",
      "Epoch [17/50], Step [80/127] Train Loss: 0.3458\n",
      "Epoch [17/50], Step [90/127] Train Loss: 0.2678\n",
      "Epoch [17/50], Step [100/127] Train Loss: 0.2281\n",
      "Epoch [17/50], Step [110/127] Train Loss: 0.2729\n",
      "Epoch [17/50], Step [120/127] Train Loss: 0.2905\n",
      "Train loss at epoch 17 is 0.3247\n",
      "Train acc at epoch 17 is 93.3062\n",
      "Val loss at epoch 17 is 0.0372\n",
      "Val acc at epoch 17 is 91.7750\n",
      "Epoch [18/50], Step [10/127] Train Loss: 0.2526\n",
      "Epoch [18/50], Step [20/127] Train Loss: 0.3372\n",
      "Epoch [18/50], Step [30/127] Train Loss: 0.3276\n",
      "Epoch [18/50], Step [40/127] Train Loss: 0.3070\n",
      "Epoch [18/50], Step [50/127] Train Loss: 0.2973\n",
      "Epoch [18/50], Step [60/127] Train Loss: 0.3302\n",
      "Epoch [18/50], Step [70/127] Train Loss: 0.3709\n",
      "Epoch [18/50], Step [80/127] Train Loss: 0.2841\n",
      "Epoch [18/50], Step [90/127] Train Loss: 0.3209\n",
      "Epoch [18/50], Step [100/127] Train Loss: 0.3589\n",
      "Epoch [18/50], Step [110/127] Train Loss: 0.3275\n",
      "Epoch [18/50], Step [120/127] Train Loss: 0.2726\n",
      "Train loss at epoch 18 is 0.3158\n",
      "Train acc at epoch 18 is 93.5408\n",
      "Val loss at epoch 18 is 0.0062\n",
      "Val acc at epoch 18 is 92.0029\n",
      "Epoch [19/50], Step [10/127] Train Loss: 0.2527\n",
      "Epoch [19/50], Step [20/127] Train Loss: 0.2320\n",
      "Epoch [19/50], Step [30/127] Train Loss: 0.2729\n",
      "Epoch [19/50], Step [40/127] Train Loss: 0.4607\n",
      "Epoch [19/50], Step [50/127] Train Loss: 0.2868\n",
      "Epoch [19/50], Step [60/127] Train Loss: 0.2912\n",
      "Epoch [19/50], Step [70/127] Train Loss: 0.3749\n",
      "Epoch [19/50], Step [80/127] Train Loss: 0.2566\n",
      "Epoch [19/50], Step [90/127] Train Loss: 0.3397\n",
      "Epoch [19/50], Step [100/127] Train Loss: 0.3174\n",
      "Epoch [19/50], Step [110/127] Train Loss: 0.3453\n",
      "Epoch [19/50], Step [120/127] Train Loss: 0.2863\n",
      "Train loss at epoch 19 is 0.3070\n",
      "Train acc at epoch 19 is 93.7261\n",
      "Val loss at epoch 19 is 0.0471\n",
      "Val acc at epoch 19 is 92.1585\n",
      "Epoch [20/50], Step [10/127] Train Loss: 0.2803\n",
      "Epoch [20/50], Step [20/127] Train Loss: 0.2205\n",
      "Epoch [20/50], Step [30/127] Train Loss: 0.3115\n",
      "Epoch [20/50], Step [40/127] Train Loss: 0.3420\n",
      "Epoch [20/50], Step [50/127] Train Loss: 0.2617\n",
      "Epoch [20/50], Step [60/127] Train Loss: 0.2177\n",
      "Epoch [20/50], Step [70/127] Train Loss: 0.2950\n",
      "Epoch [20/50], Step [80/127] Train Loss: 0.3148\n",
      "Epoch [20/50], Step [90/127] Train Loss: 0.2057\n",
      "Epoch [20/50], Step [100/127] Train Loss: 0.3140\n",
      "Epoch [20/50], Step [110/127] Train Loss: 0.2619\n",
      "Epoch [20/50], Step [120/127] Train Loss: 0.2500\n",
      "Train loss at epoch 20 is 0.2983\n",
      "Train acc at epoch 20 is 93.9669\n",
      "Val loss at epoch 20 is 0.0420\n",
      "Val acc at epoch 20 is 92.3530\n",
      "Epoch [21/50], Step [10/127] Train Loss: 0.2597\n",
      "Epoch [21/50], Step [20/127] Train Loss: 0.2694\n",
      "Epoch [21/50], Step [30/127] Train Loss: 0.2586\n",
      "Epoch [21/50], Step [40/127] Train Loss: 0.2594\n",
      "Epoch [21/50], Step [50/127] Train Loss: 0.2115\n",
      "Epoch [21/50], Step [60/127] Train Loss: 0.3149\n",
      "Epoch [21/50], Step [70/127] Train Loss: 0.2739\n",
      "Epoch [21/50], Step [80/127] Train Loss: 0.2500\n",
      "Epoch [21/50], Step [90/127] Train Loss: 0.3138\n",
      "Epoch [21/50], Step [100/127] Train Loss: 0.2350\n",
      "Epoch [21/50], Step [110/127] Train Loss: 0.2106\n",
      "Epoch [21/50], Step [120/127] Train Loss: 0.2338\n",
      "Train loss at epoch 21 is 0.2857\n",
      "Train acc at epoch 21 is 94.5412\n",
      "Val loss at epoch 21 is 0.0877\n",
      "Val acc at epoch 21 is 92.9199\n",
      "Epoch [22/50], Step [10/127] Train Loss: 0.3213\n",
      "Epoch [22/50], Step [20/127] Train Loss: 0.2587\n",
      "Epoch [22/50], Step [30/127] Train Loss: 0.3153\n",
      "Epoch [22/50], Step [40/127] Train Loss: 0.2952\n",
      "Epoch [22/50], Step [50/127] Train Loss: 0.3163\n",
      "Epoch [22/50], Step [60/127] Train Loss: 0.3225\n",
      "Epoch [22/50], Step [70/127] Train Loss: 0.2468\n",
      "Epoch [22/50], Step [80/127] Train Loss: 0.2698\n",
      "Epoch [22/50], Step [90/127] Train Loss: 0.2678\n",
      "Epoch [22/50], Step [100/127] Train Loss: 0.2939\n",
      "Epoch [22/50], Step [110/127] Train Loss: 0.2885\n",
      "Epoch [22/50], Step [120/127] Train Loss: 0.2770\n",
      "Train loss at epoch 22 is 0.2822\n",
      "Train acc at epoch 22 is 94.6832\n",
      "Val loss at epoch 22 is 0.0254\n",
      "Val acc at epoch 22 is 93.0255\n",
      "Epoch [23/50], Step [10/127] Train Loss: 0.2050\n",
      "Epoch [23/50], Step [20/127] Train Loss: 0.2355\n",
      "Epoch [23/50], Step [30/127] Train Loss: 0.3367\n",
      "Epoch [23/50], Step [40/127] Train Loss: 0.3342\n",
      "Epoch [23/50], Step [50/127] Train Loss: 0.2470\n",
      "Epoch [23/50], Step [60/127] Train Loss: 0.2842\n",
      "Epoch [23/50], Step [70/127] Train Loss: 0.2704\n",
      "Epoch [23/50], Step [80/127] Train Loss: 0.2443\n",
      "Epoch [23/50], Step [90/127] Train Loss: 0.4513\n",
      "Epoch [23/50], Step [100/127] Train Loss: 0.2434\n",
      "Epoch [23/50], Step [110/127] Train Loss: 0.2236\n",
      "Epoch [23/50], Step [120/127] Train Loss: 0.2891\n",
      "Train loss at epoch 23 is 0.2795\n",
      "Train acc at epoch 23 is 94.6956\n",
      "Val loss at epoch 23 is 0.0437\n",
      "Val acc at epoch 23 is 93.0532\n",
      "Epoch [24/50], Step [10/127] Train Loss: 0.2953\n",
      "Epoch [24/50], Step [20/127] Train Loss: 0.2995\n",
      "Epoch [24/50], Step [30/127] Train Loss: 0.2922\n",
      "Epoch [24/50], Step [40/127] Train Loss: 0.1994\n",
      "Epoch [24/50], Step [50/127] Train Loss: 0.2979\n",
      "Epoch [24/50], Step [60/127] Train Loss: 0.3193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Step [70/127] Train Loss: 0.2584\n",
      "Epoch [24/50], Step [80/127] Train Loss: 0.2807\n",
      "Epoch [24/50], Step [90/127] Train Loss: 0.3157\n",
      "Epoch [24/50], Step [100/127] Train Loss: 0.2131\n",
      "Epoch [24/50], Step [110/127] Train Loss: 0.3367\n",
      "Epoch [24/50], Step [120/127] Train Loss: 0.2992\n",
      "Train loss at epoch 24 is 0.2769\n",
      "Train acc at epoch 24 is 94.7511\n",
      "Val loss at epoch 24 is 0.0564\n",
      "Val acc at epoch 24 is 93.1088\n",
      "Epoch [25/50], Step [10/127] Train Loss: 0.1978\n",
      "Epoch [25/50], Step [20/127] Train Loss: 0.2969\n",
      "Epoch [25/50], Step [30/127] Train Loss: 0.2538\n",
      "Epoch [25/50], Step [40/127] Train Loss: 0.3143\n",
      "Epoch [25/50], Step [50/127] Train Loss: 0.2946\n",
      "Epoch [25/50], Step [60/127] Train Loss: 0.2554\n",
      "Epoch [25/50], Step [70/127] Train Loss: 0.3035\n",
      "Epoch [25/50], Step [80/127] Train Loss: 0.2723\n",
      "Epoch [25/50], Step [90/127] Train Loss: 0.2736\n",
      "Epoch [25/50], Step [100/127] Train Loss: 0.2417\n",
      "Epoch [25/50], Step [110/127] Train Loss: 0.2609\n",
      "Epoch [25/50], Step [120/127] Train Loss: 0.2188\n",
      "Train loss at epoch 25 is 0.2740\n",
      "Train acc at epoch 25 is 94.8314\n",
      "Val loss at epoch 25 is 0.0483\n",
      "Val acc at epoch 25 is 93.2033\n",
      "Epoch [26/50], Step [10/127] Train Loss: 0.3107\n",
      "Epoch [26/50], Step [20/127] Train Loss: 0.3427\n",
      "Epoch [26/50], Step [30/127] Train Loss: 0.2210\n",
      "Epoch [26/50], Step [40/127] Train Loss: 0.2251\n",
      "Epoch [26/50], Step [50/127] Train Loss: 0.2548\n",
      "Epoch [26/50], Step [60/127] Train Loss: 0.2652\n",
      "Epoch [26/50], Step [70/127] Train Loss: 0.3449\n",
      "Epoch [26/50], Step [80/127] Train Loss: 0.3154\n",
      "Epoch [26/50], Step [90/127] Train Loss: 0.3169\n",
      "Epoch [26/50], Step [100/127] Train Loss: 0.2992\n",
      "Epoch [26/50], Step [110/127] Train Loss: 0.2386\n",
      "Epoch [26/50], Step [120/127] Train Loss: 0.2448\n",
      "Train loss at epoch 26 is 0.2712\n",
      "Train acc at epoch 26 is 94.9426\n",
      "Val loss at epoch 26 is 0.0317\n",
      "Val acc at epoch 26 is 93.2589\n",
      "Epoch [27/50], Step [10/127] Train Loss: 0.2996\n",
      "Epoch [27/50], Step [20/127] Train Loss: 0.3389\n",
      "Epoch [27/50], Step [30/127] Train Loss: 0.3244\n",
      "Epoch [27/50], Step [40/127] Train Loss: 0.1963\n",
      "Epoch [27/50], Step [50/127] Train Loss: 0.2324\n",
      "Epoch [27/50], Step [60/127] Train Loss: 0.2793\n",
      "Epoch [27/50], Step [70/127] Train Loss: 0.3116\n",
      "Epoch [27/50], Step [80/127] Train Loss: 0.2945\n",
      "Epoch [27/50], Step [90/127] Train Loss: 0.3052\n",
      "Epoch [27/50], Step [100/127] Train Loss: 0.2487\n",
      "Epoch [27/50], Step [110/127] Train Loss: 0.2588\n",
      "Epoch [27/50], Step [120/127] Train Loss: 0.2580\n",
      "Train loss at epoch 27 is 0.2685\n",
      "Train acc at epoch 27 is 94.9981\n",
      "Val loss at epoch 27 is 0.0518\n",
      "Val acc at epoch 27 is 93.3256\n",
      "Epoch [28/50], Step [10/127] Train Loss: 0.2712\n",
      "Epoch [28/50], Step [20/127] Train Loss: 0.2759\n",
      "Epoch [28/50], Step [30/127] Train Loss: 0.3039\n",
      "Epoch [28/50], Step [40/127] Train Loss: 0.2855\n",
      "Epoch [28/50], Step [50/127] Train Loss: 0.2827\n",
      "Epoch [28/50], Step [60/127] Train Loss: 0.3260\n",
      "Epoch [28/50], Step [70/127] Train Loss: 0.2360\n",
      "Epoch [28/50], Step [80/127] Train Loss: 0.2999\n",
      "Epoch [28/50], Step [90/127] Train Loss: 0.2590\n",
      "Epoch [28/50], Step [100/127] Train Loss: 0.2122\n",
      "Epoch [28/50], Step [110/127] Train Loss: 0.2236\n",
      "Epoch [28/50], Step [120/127] Train Loss: 0.2271\n",
      "Train loss at epoch 28 is 0.2656\n",
      "Train acc at epoch 28 is 95.0599\n",
      "Val loss at epoch 28 is 0.0731\n",
      "Val acc at epoch 28 is 93.3811\n",
      "Epoch [29/50], Step [10/127] Train Loss: 0.3766\n",
      "Epoch [29/50], Step [20/127] Train Loss: 0.2814\n",
      "Epoch [29/50], Step [30/127] Train Loss: 0.2119\n",
      "Epoch [29/50], Step [40/127] Train Loss: 0.2169\n",
      "Epoch [29/50], Step [50/127] Train Loss: 0.2617\n",
      "Epoch [29/50], Step [60/127] Train Loss: 0.2733\n",
      "Epoch [29/50], Step [70/127] Train Loss: 0.2776\n",
      "Epoch [29/50], Step [80/127] Train Loss: 0.2418\n",
      "Epoch [29/50], Step [90/127] Train Loss: 0.2341\n",
      "Epoch [29/50], Step [100/127] Train Loss: 0.2606\n",
      "Epoch [29/50], Step [110/127] Train Loss: 0.2627\n",
      "Epoch [29/50], Step [120/127] Train Loss: 0.2545\n",
      "Train loss at epoch 29 is 0.2629\n",
      "Train acc at epoch 29 is 95.1278\n",
      "Val loss at epoch 29 is 0.1321\n",
      "Val acc at epoch 29 is 93.4367\n",
      "Epoch [30/50], Step [10/127] Train Loss: 0.2095\n",
      "Epoch [30/50], Step [20/127] Train Loss: 0.3003\n",
      "Epoch [30/50], Step [30/127] Train Loss: 0.2105\n",
      "Epoch [30/50], Step [40/127] Train Loss: 0.2248\n",
      "Epoch [30/50], Step [50/127] Train Loss: 0.2468\n",
      "Epoch [30/50], Step [60/127] Train Loss: 0.2737\n",
      "Epoch [30/50], Step [70/127] Train Loss: 0.2695\n",
      "Epoch [30/50], Step [80/127] Train Loss: 0.2482\n",
      "Epoch [30/50], Step [90/127] Train Loss: 0.2946\n",
      "Epoch [30/50], Step [100/127] Train Loss: 0.1839\n",
      "Epoch [30/50], Step [110/127] Train Loss: 0.2083\n",
      "Epoch [30/50], Step [120/127] Train Loss: 0.2578\n",
      "Train loss at epoch 30 is 0.2604\n",
      "Train acc at epoch 30 is 95.2884\n",
      "Val loss at epoch 30 is 0.1501\n",
      "Val acc at epoch 30 is 93.6201\n",
      "Epoch [31/50], Step [10/127] Train Loss: 0.1962\n",
      "Epoch [31/50], Step [20/127] Train Loss: 0.2552\n",
      "Epoch [31/50], Step [30/127] Train Loss: 0.2795\n",
      "Epoch [31/50], Step [40/127] Train Loss: 0.2600\n",
      "Epoch [31/50], Step [50/127] Train Loss: 0.2321\n",
      "Epoch [31/50], Step [60/127] Train Loss: 0.2389\n",
      "Epoch [31/50], Step [70/127] Train Loss: 0.2334\n",
      "Epoch [31/50], Step [80/127] Train Loss: 0.2169\n",
      "Epoch [31/50], Step [90/127] Train Loss: 0.1960\n",
      "Epoch [31/50], Step [100/127] Train Loss: 0.2165\n",
      "Epoch [31/50], Step [110/127] Train Loss: 0.2348\n",
      "Epoch [31/50], Step [120/127] Train Loss: 0.2217\n",
      "Train loss at epoch 31 is 0.2560\n",
      "Train acc at epoch 31 is 95.2637\n",
      "Val loss at epoch 31 is 0.0499\n",
      "Val acc at epoch 31 is 93.5756\n",
      "Epoch [32/50], Step [10/127] Train Loss: 0.1849\n",
      "Epoch [32/50], Step [20/127] Train Loss: 0.2925\n",
      "Epoch [32/50], Step [30/127] Train Loss: 0.2518\n",
      "Epoch [32/50], Step [40/127] Train Loss: 0.2187\n",
      "Epoch [32/50], Step [50/127] Train Loss: 0.2584\n",
      "Epoch [32/50], Step [60/127] Train Loss: 0.2582\n",
      "Epoch [32/50], Step [70/127] Train Loss: 0.2533\n",
      "Epoch [32/50], Step [80/127] Train Loss: 0.2767\n",
      "Epoch [32/50], Step [90/127] Train Loss: 0.2574\n",
      "Epoch [32/50], Step [100/127] Train Loss: 0.2612\n",
      "Epoch [32/50], Step [110/127] Train Loss: 0.2052\n",
      "Epoch [32/50], Step [120/127] Train Loss: 0.1843\n",
      "Train loss at epoch 32 is 0.2549\n",
      "Train acc at epoch 32 is 95.3810\n",
      "Val loss at epoch 32 is 0.0909\n",
      "Val acc at epoch 32 is 93.6812\n",
      "Epoch [33/50], Step [10/127] Train Loss: 0.3364\n",
      "Epoch [33/50], Step [20/127] Train Loss: 0.2470\n",
      "Epoch [33/50], Step [30/127] Train Loss: 0.2729\n",
      "Epoch [33/50], Step [40/127] Train Loss: 0.2306\n",
      "Epoch [33/50], Step [50/127] Train Loss: 0.2373\n",
      "Epoch [33/50], Step [60/127] Train Loss: 0.2101\n",
      "Epoch [33/50], Step [70/127] Train Loss: 0.2379\n",
      "Epoch [33/50], Step [80/127] Train Loss: 0.2908\n",
      "Epoch [33/50], Step [90/127] Train Loss: 0.2651\n",
      "Epoch [33/50], Step [100/127] Train Loss: 0.2599\n",
      "Epoch [33/50], Step [110/127] Train Loss: 0.1997\n",
      "Epoch [33/50], Step [120/127] Train Loss: 0.2255\n",
      "Train loss at epoch 33 is 0.2539\n",
      "Train acc at epoch 33 is 95.3687\n",
      "Val loss at epoch 33 is 0.0199\n",
      "Val acc at epoch 33 is 93.6646\n",
      "Epoch [34/50], Step [10/127] Train Loss: 0.2662\n",
      "Epoch [34/50], Step [20/127] Train Loss: 0.2744\n",
      "Epoch [34/50], Step [30/127] Train Loss: 0.1680\n",
      "Epoch [34/50], Step [40/127] Train Loss: 0.2866\n",
      "Epoch [34/50], Step [50/127] Train Loss: 0.2752\n",
      "Epoch [34/50], Step [60/127] Train Loss: 0.3048\n",
      "Epoch [34/50], Step [70/127] Train Loss: 0.2062\n",
      "Epoch [34/50], Step [80/127] Train Loss: 0.2470\n",
      "Epoch [34/50], Step [90/127] Train Loss: 0.2710\n",
      "Epoch [34/50], Step [100/127] Train Loss: 0.2407\n",
      "Epoch [34/50], Step [110/127] Train Loss: 0.2404\n",
      "Epoch [34/50], Step [120/127] Train Loss: 0.2447\n",
      "Train loss at epoch 34 is 0.2529\n",
      "Train acc at epoch 34 is 95.4242\n",
      "Val loss at epoch 34 is 0.0329\n",
      "Val acc at epoch 34 is 93.6979\n",
      "Epoch [35/50], Step [10/127] Train Loss: 0.3440\n",
      "Epoch [35/50], Step [20/127] Train Loss: 0.1768\n",
      "Epoch [35/50], Step [30/127] Train Loss: 0.2243\n",
      "Epoch [35/50], Step [40/127] Train Loss: 0.2249\n",
      "Epoch [35/50], Step [50/127] Train Loss: 0.2756\n",
      "Epoch [35/50], Step [60/127] Train Loss: 0.2031\n",
      "Epoch [35/50], Step [70/127] Train Loss: 0.2100\n",
      "Epoch [35/50], Step [80/127] Train Loss: 0.2420\n",
      "Epoch [35/50], Step [90/127] Train Loss: 0.2316\n",
      "Epoch [35/50], Step [100/127] Train Loss: 0.3544\n",
      "Epoch [35/50], Step [110/127] Train Loss: 0.2343\n",
      "Epoch [35/50], Step [120/127] Train Loss: 0.2668\n",
      "Train loss at epoch 35 is 0.2520\n",
      "Train acc at epoch 35 is 95.4489\n",
      "Val loss at epoch 35 is 0.0407\n",
      "Val acc at epoch 35 is 93.7368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Step [10/127] Train Loss: 0.1958\n",
      "Epoch [36/50], Step [20/127] Train Loss: 0.2839\n",
      "Epoch [36/50], Step [30/127] Train Loss: 0.2070\n",
      "Epoch [36/50], Step [40/127] Train Loss: 0.2448\n",
      "Epoch [36/50], Step [50/127] Train Loss: 0.2654\n",
      "Epoch [36/50], Step [60/127] Train Loss: 0.3023\n",
      "Epoch [36/50], Step [70/127] Train Loss: 0.2474\n",
      "Epoch [36/50], Step [80/127] Train Loss: 0.2694\n",
      "Epoch [36/50], Step [90/127] Train Loss: 0.2502\n",
      "Epoch [36/50], Step [100/127] Train Loss: 0.3562\n",
      "Epoch [36/50], Step [110/127] Train Loss: 0.2545\n",
      "Epoch [36/50], Step [120/127] Train Loss: 0.2213\n",
      "Train loss at epoch 36 is 0.2512\n",
      "Train acc at epoch 36 is 95.4860\n",
      "Val loss at epoch 36 is 0.0180\n",
      "Val acc at epoch 36 is 93.7479\n",
      "Epoch [37/50], Step [10/127] Train Loss: 0.2900\n",
      "Epoch [37/50], Step [20/127] Train Loss: 0.2515\n",
      "Epoch [37/50], Step [30/127] Train Loss: 0.2526\n",
      "Epoch [37/50], Step [40/127] Train Loss: 0.2450\n",
      "Epoch [37/50], Step [50/127] Train Loss: 0.3370\n",
      "Epoch [37/50], Step [60/127] Train Loss: 0.2684\n",
      "Epoch [37/50], Step [70/127] Train Loss: 0.2276\n",
      "Epoch [37/50], Step [80/127] Train Loss: 0.2527\n",
      "Epoch [37/50], Step [90/127] Train Loss: 0.3297\n",
      "Epoch [37/50], Step [100/127] Train Loss: 0.2760\n",
      "Epoch [37/50], Step [110/127] Train Loss: 0.2381\n",
      "Epoch [37/50], Step [120/127] Train Loss: 0.2229\n",
      "Train loss at epoch 37 is 0.2499\n",
      "Train acc at epoch 37 is 95.5169\n",
      "Val loss at epoch 37 is 0.0136\n",
      "Val acc at epoch 37 is 93.7813\n",
      "Epoch [38/50], Step [10/127] Train Loss: 0.2401\n",
      "Epoch [38/50], Step [20/127] Train Loss: 0.2981\n",
      "Epoch [38/50], Step [30/127] Train Loss: 0.2163\n",
      "Epoch [38/50], Step [40/127] Train Loss: 0.2559\n",
      "Epoch [38/50], Step [50/127] Train Loss: 0.2151\n",
      "Epoch [38/50], Step [60/127] Train Loss: 0.1799\n",
      "Epoch [38/50], Step [70/127] Train Loss: 0.2719\n",
      "Epoch [38/50], Step [80/127] Train Loss: 0.2737\n",
      "Epoch [38/50], Step [90/127] Train Loss: 0.2712\n",
      "Epoch [38/50], Step [100/127] Train Loss: 0.2549\n",
      "Epoch [38/50], Step [110/127] Train Loss: 0.2575\n",
      "Epoch [38/50], Step [120/127] Train Loss: 0.2488\n",
      "Train loss at epoch 38 is 0.2497\n",
      "Train acc at epoch 38 is 95.5971\n",
      "Val loss at epoch 38 is 0.0541\n",
      "Val acc at epoch 38 is 93.8702\n",
      "Epoch [39/50], Step [10/127] Train Loss: 0.1568\n",
      "Epoch [39/50], Step [20/127] Train Loss: 0.2515\n",
      "Epoch [39/50], Step [30/127] Train Loss: 0.1836\n",
      "Epoch [39/50], Step [40/127] Train Loss: 0.2452\n",
      "Epoch [39/50], Step [50/127] Train Loss: 0.2555\n",
      "Epoch [39/50], Step [60/127] Train Loss: 0.2436\n",
      "Epoch [39/50], Step [70/127] Train Loss: 0.1984\n",
      "Epoch [39/50], Step [80/127] Train Loss: 0.2494\n",
      "Epoch [39/50], Step [90/127] Train Loss: 0.2147\n",
      "Epoch [39/50], Step [100/127] Train Loss: 0.2917\n",
      "Epoch [39/50], Step [110/127] Train Loss: 0.2113\n",
      "Epoch [39/50], Step [120/127] Train Loss: 0.2351\n",
      "Train loss at epoch 39 is 0.2487\n",
      "Train acc at epoch 39 is 95.5292\n",
      "Val loss at epoch 39 is 0.0442\n",
      "Val acc at epoch 39 is 93.8090\n",
      "Epoch [40/50], Step [10/127] Train Loss: 0.2902\n",
      "Epoch [40/50], Step [20/127] Train Loss: 0.2234\n",
      "Epoch [40/50], Step [30/127] Train Loss: 0.2456\n",
      "Epoch [40/50], Step [40/127] Train Loss: 0.2689\n",
      "Epoch [40/50], Step [50/127] Train Loss: 0.3093\n",
      "Epoch [40/50], Step [60/127] Train Loss: 0.2781\n",
      "Epoch [40/50], Step [70/127] Train Loss: 0.2257\n",
      "Epoch [40/50], Step [80/127] Train Loss: 0.2314\n",
      "Epoch [40/50], Step [90/127] Train Loss: 0.2201\n",
      "Epoch [40/50], Step [100/127] Train Loss: 0.2986\n",
      "Epoch [40/50], Step [110/127] Train Loss: 0.2089\n",
      "Epoch [40/50], Step [120/127] Train Loss: 0.2145\n",
      "Train loss at epoch 40 is 0.2474\n",
      "Train acc at epoch 40 is 95.5786\n",
      "Val loss at epoch 40 is 0.0283\n",
      "Val acc at epoch 40 is 93.8535\n",
      "Epoch [41/50], Step [10/127] Train Loss: 0.2362\n",
      "Epoch [41/50], Step [20/127] Train Loss: 0.2425\n",
      "Epoch [41/50], Step [30/127] Train Loss: 0.2440\n",
      "Epoch [41/50], Step [40/127] Train Loss: 0.2276\n",
      "Epoch [41/50], Step [50/127] Train Loss: 0.2548\n",
      "Epoch [41/50], Step [60/127] Train Loss: 0.2557\n",
      "Epoch [41/50], Step [70/127] Train Loss: 0.2944\n",
      "Epoch [41/50], Step [80/127] Train Loss: 0.2196\n",
      "Epoch [41/50], Step [90/127] Train Loss: 0.2272\n",
      "Epoch [41/50], Step [100/127] Train Loss: 0.2609\n",
      "Epoch [41/50], Step [110/127] Train Loss: 0.2760\n",
      "Epoch [41/50], Step [120/127] Train Loss: 0.2468\n",
      "Train loss at epoch 41 is 0.2456\n",
      "Train acc at epoch 41 is 95.6095\n",
      "Val loss at epoch 41 is 0.0471\n",
      "Val acc at epoch 41 is 93.8813\n",
      "Epoch [42/50], Step [10/127] Train Loss: 0.2298\n",
      "Epoch [42/50], Step [20/127] Train Loss: 0.1858\n",
      "Epoch [42/50], Step [30/127] Train Loss: 0.2548\n",
      "Epoch [42/50], Step [40/127] Train Loss: 0.2239\n",
      "Epoch [42/50], Step [50/127] Train Loss: 0.2575\n",
      "Epoch [42/50], Step [60/127] Train Loss: 0.2222\n",
      "Epoch [42/50], Step [70/127] Train Loss: 0.2355\n",
      "Epoch [42/50], Step [80/127] Train Loss: 0.3106\n",
      "Epoch [42/50], Step [90/127] Train Loss: 0.2342\n",
      "Epoch [42/50], Step [100/127] Train Loss: 0.2385\n",
      "Epoch [42/50], Step [110/127] Train Loss: 0.2252\n",
      "Epoch [42/50], Step [120/127] Train Loss: 0.2850\n",
      "Train loss at epoch 42 is 0.2456\n",
      "Train acc at epoch 42 is 95.6218\n",
      "Val loss at epoch 42 is 0.0187\n",
      "Val acc at epoch 42 is 93.8924\n",
      "Epoch [43/50], Step [10/127] Train Loss: 0.2531\n",
      "Epoch [43/50], Step [20/127] Train Loss: 0.2482\n",
      "Epoch [43/50], Step [30/127] Train Loss: 0.2410\n",
      "Epoch [43/50], Step [40/127] Train Loss: 0.2334\n",
      "Epoch [43/50], Step [50/127] Train Loss: 0.2900\n",
      "Epoch [43/50], Step [60/127] Train Loss: 0.2330\n",
      "Epoch [43/50], Step [70/127] Train Loss: 0.2145\n",
      "Epoch [43/50], Step [80/127] Train Loss: 0.2557\n",
      "Epoch [43/50], Step [90/127] Train Loss: 0.2290\n",
      "Epoch [43/50], Step [100/127] Train Loss: 0.2803\n",
      "Epoch [43/50], Step [110/127] Train Loss: 0.2400\n",
      "Epoch [43/50], Step [120/127] Train Loss: 0.2370\n",
      "Train loss at epoch 43 is 0.2450\n",
      "Train acc at epoch 43 is 95.6836\n",
      "Val loss at epoch 43 is 0.1281\n",
      "Val acc at epoch 43 is 93.9480\n",
      "Epoch [44/50], Step [10/127] Train Loss: 0.2639\n",
      "Epoch [44/50], Step [20/127] Train Loss: 0.2779\n",
      "Epoch [44/50], Step [30/127] Train Loss: 0.2181\n",
      "Epoch [44/50], Step [40/127] Train Loss: 0.2108\n",
      "Epoch [44/50], Step [50/127] Train Loss: 0.2807\n",
      "Epoch [44/50], Step [60/127] Train Loss: 0.2525\n",
      "Epoch [44/50], Step [70/127] Train Loss: 0.2643\n",
      "Epoch [44/50], Step [80/127] Train Loss: 0.2183\n",
      "Epoch [44/50], Step [90/127] Train Loss: 0.2509\n",
      "Epoch [44/50], Step [100/127] Train Loss: 0.2894\n",
      "Epoch [44/50], Step [110/127] Train Loss: 0.2788\n",
      "Epoch [44/50], Step [120/127] Train Loss: 0.2863\n",
      "Train loss at epoch 44 is 0.2447\n",
      "Train acc at epoch 44 is 95.6280\n",
      "Val loss at epoch 44 is 0.0589\n",
      "Val acc at epoch 44 is 93.8924\n",
      "Epoch [45/50], Step [10/127] Train Loss: 0.2090\n",
      "Epoch [45/50], Step [20/127] Train Loss: 0.2534\n",
      "Epoch [45/50], Step [30/127] Train Loss: 0.2072\n",
      "Epoch [45/50], Step [40/127] Train Loss: 0.1779\n",
      "Epoch [45/50], Step [50/127] Train Loss: 0.2018\n",
      "Epoch [45/50], Step [60/127] Train Loss: 0.2678\n",
      "Epoch [45/50], Step [70/127] Train Loss: 0.2272\n",
      "Epoch [45/50], Step [80/127] Train Loss: 0.1930\n",
      "Epoch [45/50], Step [90/127] Train Loss: 0.2610\n",
      "Epoch [45/50], Step [100/127] Train Loss: 0.2243\n",
      "Epoch [45/50], Step [110/127] Train Loss: 0.1537\n",
      "Epoch [45/50], Step [120/127] Train Loss: 0.3539\n",
      "Train loss at epoch 45 is 0.2448\n",
      "Train acc at epoch 45 is 95.6465\n",
      "Val loss at epoch 45 is 0.0798\n",
      "Val acc at epoch 45 is 93.9146\n",
      "Epoch [46/50], Step [10/127] Train Loss: 0.2091\n",
      "Epoch [46/50], Step [20/127] Train Loss: 0.1861\n",
      "Epoch [46/50], Step [30/127] Train Loss: 0.2233\n",
      "Epoch [46/50], Step [40/127] Train Loss: 0.2659\n",
      "Epoch [46/50], Step [50/127] Train Loss: 0.3228\n",
      "Epoch [46/50], Step [60/127] Train Loss: 0.2081\n",
      "Epoch [46/50], Step [70/127] Train Loss: 0.2395\n",
      "Epoch [46/50], Step [80/127] Train Loss: 0.2345\n",
      "Epoch [46/50], Step [90/127] Train Loss: 0.3023\n",
      "Epoch [46/50], Step [100/127] Train Loss: 0.1977\n",
      "Epoch [46/50], Step [110/127] Train Loss: 0.2158\n",
      "Epoch [46/50], Step [120/127] Train Loss: 0.2351\n",
      "Train loss at epoch 46 is 0.2439\n",
      "Train acc at epoch 46 is 95.6465\n",
      "Val loss at epoch 46 is 0.0333\n",
      "Val acc at epoch 46 is 93.9091\n",
      "Epoch [47/50], Step [10/127] Train Loss: 0.2376\n",
      "Epoch [47/50], Step [20/127] Train Loss: 0.2238\n",
      "Epoch [47/50], Step [30/127] Train Loss: 0.1988\n",
      "Epoch [47/50], Step [40/127] Train Loss: 0.2344\n",
      "Epoch [47/50], Step [50/127] Train Loss: 0.2955\n",
      "Epoch [47/50], Step [60/127] Train Loss: 0.2847\n",
      "Epoch [47/50], Step [70/127] Train Loss: 0.2299\n",
      "Epoch [47/50], Step [80/127] Train Loss: 0.2457\n",
      "Epoch [47/50], Step [90/127] Train Loss: 0.2340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Step [100/127] Train Loss: 0.1675\n",
      "Epoch [47/50], Step [110/127] Train Loss: 0.2064\n",
      "Epoch [47/50], Step [120/127] Train Loss: 0.1969\n",
      "Train loss at epoch 47 is 0.2443\n",
      "Train acc at epoch 47 is 95.6774\n",
      "Val loss at epoch 47 is 0.0964\n",
      "Val acc at epoch 47 is 93.9369\n",
      "Epoch [48/50], Step [10/127] Train Loss: 0.2461\n",
      "Epoch [48/50], Step [20/127] Train Loss: 0.3148\n",
      "Epoch [48/50], Step [30/127] Train Loss: 0.2478\n",
      "Epoch [48/50], Step [40/127] Train Loss: 0.1927\n",
      "Epoch [48/50], Step [50/127] Train Loss: 0.2410\n",
      "Epoch [48/50], Step [60/127] Train Loss: 0.2635\n",
      "Epoch [48/50], Step [70/127] Train Loss: 0.2793\n",
      "Epoch [48/50], Step [80/127] Train Loss: 0.2034\n",
      "Epoch [48/50], Step [90/127] Train Loss: 0.2069\n",
      "Epoch [48/50], Step [100/127] Train Loss: 0.2339\n",
      "Epoch [48/50], Step [110/127] Train Loss: 0.2845\n",
      "Epoch [48/50], Step [120/127] Train Loss: 0.2442\n",
      "Train loss at epoch 48 is 0.2439\n",
      "Train acc at epoch 48 is 95.7083\n",
      "Val loss at epoch 48 is 0.0877\n",
      "Val acc at epoch 48 is 93.9647\n",
      "Epoch [49/50], Step [10/127] Train Loss: 0.3013\n",
      "Epoch [49/50], Step [20/127] Train Loss: 0.2752\n",
      "Epoch [49/50], Step [30/127] Train Loss: 0.3228\n",
      "Epoch [49/50], Step [40/127] Train Loss: 0.2012\n",
      "Epoch [49/50], Step [50/127] Train Loss: 0.1950\n",
      "Epoch [49/50], Step [60/127] Train Loss: 0.2535\n",
      "Epoch [49/50], Step [70/127] Train Loss: 0.2409\n",
      "Epoch [49/50], Step [80/127] Train Loss: 0.2113\n",
      "Epoch [49/50], Step [90/127] Train Loss: 0.2770\n",
      "Epoch [49/50], Step [100/127] Train Loss: 0.2605\n",
      "Epoch [49/50], Step [110/127] Train Loss: 0.2078\n",
      "Epoch [49/50], Step [120/127] Train Loss: 0.2754\n",
      "Train loss at epoch 49 is 0.2432\n",
      "Train acc at epoch 49 is 95.7021\n",
      "Val loss at epoch 49 is 0.0646\n",
      "Val acc at epoch 49 is 93.9591\n",
      "Epoch [50/50], Step [10/127] Train Loss: 0.2640\n",
      "Epoch [50/50], Step [20/127] Train Loss: 0.2403\n",
      "Epoch [50/50], Step [30/127] Train Loss: 0.3574\n",
      "Epoch [50/50], Step [40/127] Train Loss: 0.1906\n",
      "Epoch [50/50], Step [50/127] Train Loss: 0.2751\n",
      "Epoch [50/50], Step [60/127] Train Loss: 0.3218\n",
      "Epoch [50/50], Step [70/127] Train Loss: 0.1976\n",
      "Epoch [50/50], Step [80/127] Train Loss: 0.2441\n",
      "Epoch [50/50], Step [90/127] Train Loss: 0.2005\n",
      "Epoch [50/50], Step [100/127] Train Loss: 0.2116\n",
      "Epoch [50/50], Step [110/127] Train Loss: 0.2431\n",
      "Epoch [50/50], Step [120/127] Train Loss: 0.2387\n",
      "Train loss at epoch 50 is 0.2432\n",
      "Train acc at epoch 50 is 95.7392\n",
      "Val loss at epoch 50 is 0.1506\n",
      "Val acc at epoch 50 is 93.9924\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 50\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "# val_losses = np.array([])\n",
    "# train_losses = np.array([])\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "val_acc = []\n",
    "train_acc = []\n",
    "\n",
    "myfile = open('myfile_txt', 'w')\n",
    "myfile.write('Training results: \\n')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    correct=0\n",
    "    total=0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # print(images.shape)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimizes\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Train Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    acc = 100.*correct/total\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc.append(acc)\n",
    "    print('Train loss at epoch {} is {:.4f}'.format(epoch+1, train_loss))\n",
    "    print('Train acc at epoch {} is {:.4f}'.format(epoch+1, acc))\n",
    "    myfile.write('Train loss at epoch {} is {:.4f}\\n'.format(epoch+1, train_loss))\n",
    "    myfile.write('Train acc at epoch {} is {:.4f}\\n'.format(epoch+1, acc))\n",
    "    myfile.write('---------------------------')\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = loss / len(val_loader) \n",
    "    acc = 100.*correct/total\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc.append(acc)\n",
    "    print('Val loss at epoch {} is {:.4f}'.format(epoch+1, val_loss))\n",
    "    print('Val acc at epoch {} is {:.4f}'.format(epoch+1, acc))\n",
    "    myfile.write('Val loss at epoch {} is {:.4f}\\n'.format(epoch+1, val_loss))\n",
    "    myfile.write('Val acc at epoch {} is {:.4f}\\n'.format(epoch+1, acc))    \n",
    "    myfile.write('---------------------------\\n')\n",
    "    myfile.write('---------------------------\\n')\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300c11e",
   "metadata": {},
   "source": [
    "### Way to get information about class names --> Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87d9a76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b907832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'model_checkpoints/model_resnetv2_cub_dog_epoch50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "635aa2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 95.77003828578486 %\n"
     ]
    }
   ],
   "source": [
    " # Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2a6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cc289c304aa67a903225ac755337f14ca9e86f378cf8d96233e0cc690cb604b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
